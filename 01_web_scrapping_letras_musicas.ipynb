{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/dev/venvs/p36/lib/python3.6/site-packages/sklearn/utils/fixes.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if 'order' in inspect.getargspec(np.copy)[0]:\n",
      "/Users/daniel/dev/venvs/p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "import re \n",
    "import requests\n",
    "import os\n",
    "import nltk \n",
    "from sklearn import feature_extraction\n",
    "from gensim import models\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM, Embedding, Bidirectional\n",
    "from keras.callbacks import ModelCheckpoint, LambdaCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faz uma requisição na página que contém mil links de letras de músicas sertanejas\n",
    "url_musicas = \"https://www.letras.mus.br/mais-acessadas/sertanejo/\"\n",
    "pagina = requests.get(url_musicas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guarda o conteúdo da página\n",
    "conteudo = pagina.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o parser HTML\n",
    "soup = BeautifulSoup(conteudo,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_musicas = soup.find(\"ol\",attrs={\"class\" : \"top-list_mus\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_musicas = top_musicas.findAll(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_musicas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raiz_site = \"https://www.letras.mus.br/\"\n",
    "for link in links_musicas:\n",
    "    pagina_musica = requests.get(raiz_site + link.get(\"href\"))\n",
    "    soup_musica = BeautifulSoup(pagina_musica.content,\"html.parser\")\n",
    "    artigo_letra = soup_musica.find(\"article\")\n",
    "    nome_arquivo = str(link.get(\"href\")).replace(\"/\",\"\") + \".html\"\n",
    "    with open(\"dados/letras-musicas/\" + nome_arquivo,\"w\") as f:\n",
    "        f.write(str(artigo_letra))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentos = []\n",
    "arquivos = os.listdir(\"dados/letras-musicas\")\n",
    "for arquivo in arquivos:\n",
    "    caminho = \"dados/letras-musicas/\" + arquivo\n",
    "    if os.path.isfile(caminho):\n",
    "        with open(caminho,\"r\") as f:\n",
    "            html = f.read()\n",
    "        html = feature_extraction.text.strip_accents_ascii(html.lower())\n",
    "        html = html.replace(\"<br/>\",\" NLINHA \")\n",
    "        html = html.replace(\"<p>\",\" NLINHA \")\n",
    "        soup = BeautifulSoup(html,\"html.parser\")\n",
    "        documentos.append(soup.text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_tokenizados = []\n",
    "tokenizador = nltk.TreebankWordTokenizer()\n",
    "for doc in documentos:\n",
    "    tokens = tokenizador.tokenize(doc)\n",
    "    docs_tokenizados.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_tokenizados[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinando um modelo Word2vec nos documentos\n",
    "w2v_model = models.Word2Vec(docs_tokenizados, size=350, window=5, min_count=0, workers=os.cpu_count(),iter=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv[\"amor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um array com o vetor das duas mil primeiras palavras\n",
    "vocab = list(w2v_model.wv.vocab)\n",
    "vetores_w2v = w2v_model.wv[vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vetores_w2v = np.asfarray(vetores_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduzindo a dimensionalidade dos vetores das palavras\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2)\n",
    "vetores_reduzidos = tsne.fit_transform(vetores_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um dataframe com as colunas x e y. O objetivo é transformar cada representação de uma palavra em uma coordenada.\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(vetores_reduzidos, index=vocab, columns=['x', 'y'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utiliza o matplotlib com a chave de interação ligada.\n",
    "%matplotlib notebook\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "for word, pos in df.iterrows():\n",
    "    ax.annotate(word, pos) # Coloca a palavra em cada ponto\n",
    "\n",
    "ax.scatter(df['x'], df['y']) # Cria um scatterPlot com todos os pontos x e y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.save(\"modelos/w2v_sertanejo_1000musicas.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcoes que retornam o índice da palavra no vocabulário criado pelo word2vec e vice-versa\n",
    "def word2idx(word):\n",
    "  return w2v_model.wv.vocab[word].index\n",
    "\n",
    "def idx2word(idx):\n",
    "  return w2v_model.wv.index2word[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 5\n",
    "sentences = []\n",
    "next_tokens = []\n",
    "\n",
    "for doc in docs_tokenizados: # Cria as \"sentencas\" para cada documento do corpus\n",
    "    for i in range(0,len(doc) - maxlen):\n",
    "        sentences.append(doc[i:i+maxlen])\n",
    "        next_tokens.append(doc[i+maxlen])\n",
    "\n",
    "print('Number of sequences:', len(sentences), \"\\n\")\n",
    "\n",
    "# sentences = sentences[0:30000] # Limitando em 30 mil sentenças para não demorar...\n",
    "# next_tokens = next_tokens[0:30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando os inputs(x) e targets(y)\n",
    "x = np.zeros((len(sentences), maxlen), dtype=np.int32)\n",
    "y = np.zeros((len(sentences)), dtype=np.int32)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, token in enumerate(sentence):\n",
    "        x[i, t] = word2idx(token) \n",
    "    y[i] = word2idx(next_tokens[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = w2v_model.wv.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tamanho_vocab = pretrained_weights.shape[0]\n",
    "tamanho_vetor_w2v = pretrained_weights.shape[1] # 350\n",
    "print(\"Tamanho vocab e w2v vector: \", (tamanho_vocab, tamanho_vetor_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo o modelo LSTM\n",
    "model = Sequential()\n",
    "# A cama de de Embedding é onde podemos passar para a LSTM os vetores já treinados pelo word2vec\n",
    "model.add(Embedding(input_dim= tamanho_vocab, output_dim=tamanho_vetor_w2v, weights=[pretrained_weights]))\n",
    "model.add(Bidirectional(LSTM(350, activation=\"relu\", return_sequences=True)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(LSTM(units=280))\n",
    "# model.add(LSTM(64, input_shape=(pretrained_weights.shape[0], pretrained_weights.shape[1]) ))\n",
    "# model.add(Dropout(0.1))\n",
    "model.add(Dense(tamanho_vocab, activation='softmax')) # Quantidade de 'respostas' possíveis. Tokens neste caso. \n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "caminho_modelo_lstm =\"modelos/bilstm-w2v-wordlevel-350-280-sertanejo.model\"\n",
    "checkpoint = ModelCheckpoint(caminho_modelo_lstm, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerar_texto(epoch, logs):\n",
    "    \n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "    \n",
    "    # pick a random seed\n",
    "    start = np.random.randint(0, len(sentences))\n",
    "    seed_tokens = list(sentences[start])\n",
    "    print (\"Seed:\")\n",
    "    print (\"\\\"\", \" \".join(seed_tokens), \"\\\"\\n\")\n",
    "    \n",
    "    # generate characters\n",
    "    for i in range(500):\n",
    "        \n",
    "        xt = np.zeros((1, maxlen), dtype=np.int32)\n",
    "        \n",
    "        for t, token in enumerate(seed_tokens):\n",
    "            xt[0,t] = word2idx(token)\n",
    "        \n",
    "        prediction = model.predict(xt, verbose=0)\n",
    "#         ordered = (-prediction[0]).argsort()[:1]\n",
    "#         print(len(ordered),ordered)\n",
    "#         for x in ordered:\n",
    "#             print(prediction[0][x])\n",
    "#         indice_aleatorio = np.random.choice(ordered)\n",
    "        index = np.argmax(prediction)\n",
    "        result = idx2word(index)\n",
    "        print(\"\\n\" if result == \"NLINHA\" else result,end=\" \")\n",
    "        seed_tokens.append(result)\n",
    "        seed_tokens = seed_tokens[1:len(seed_tokens)]\n",
    "        \n",
    "    print(\"\\n\\nFIM\\n\\n\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback para gerar texto de exemplo ao final de cada iteração de treino\n",
    "print_callback = LambdaCallback(on_epoch_end=gerar_texto)\n",
    "callbacks_list.append(print_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x, y, epochs=30, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gerar_texto(1,\"teste\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
